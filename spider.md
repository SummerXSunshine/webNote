## 爬虫:

>  爬虫：使用任何技术手段，批量获取网站信息的一种方式。

### 爬虫的分类

​	按爬虫功能，可以分为网页爬虫和接口爬虫。

​	接口爬虫：通过精准构造特定 API 接口的请求数据，而获得大量数据信息。

​	自动化工具爬虫: 通过自动化工具selenium,splash等自动化工具模拟真实的用户进行爬取

### 对手（爬虫）是哪些人?

​	爬虫和反爬虫的斗争由来已久，要想做好反爬虫，先要知道对手有哪些，才好制定相应的策略。反爬虫工程师的对手通常来自以下几类：

**应届毕业生**

​	每年三月份左右通常会有一波爬虫高峰，和应届毕业生（本科、硕士、博士）有关，为了让论文有数据支撑，他们的爬虫简单粗暴，忽略了服务器压力，且人数不可预测。

**创业小公司：**

​	初创公司缺少数据支撑，为了公司生存问题，爬取别家数据，不过通常持续不久，较易被反爬虫手段逼退。

**成型的商业对手：**

​	反爬虫工作最大的对手，有钱有人有技术，如果需要，会通过分布式、跨省机房、ADSL 等种种手段进行长期爬取。如果双方持续对抗，最终的结果可能会是彼此找到平衡点。

**失控爬虫：**

​	许多爬虫放于服务器运行后，就被程序员忘了，它们或许早已爬不到数据了，但依然会孜孜不倦的消耗服务器资源，直到爬虫所在服务器到期。

## 反爬虫:

> 反爬虫：使用任何技术手段，阻止别人批量获取自己网站信息的一种方式。

### 反爬的几种做法

1. 信息校验反爬虫
2. IP访问频率限制
3. 蜜罐技术
4. 验证码,滑动解锁
5. 设备指纹
6. 文本混淆反爬虫-iconfont字体反爬
7. 代码混淆

#### 1.信息校验反爬

##### header知识

- host：提供了主机名及端口号
- Referer： 提供给服务器客户端从那个页面链接过来的信息（有些网站会据此来反爬）
- Origin：Origin字段里只包含是谁发起的请求，并没有其他信息.(仅存于post请求)
- User agent: 发送请求的应用程序名（一些网站会根据UA访问的频率间隔时间进行反爬）
- proxies： 代理，一些网站会根据ip访问的频率次数等选择封ip.
- cookie： 特定的标记信息，一般可以直接复制，对于一些变化的可以选择构造.

##### User-Agent反爬虫

​	之所以选择User-Agent头域作为校验对象，是因为很多编程语言和软件有默认的标识。在发起网络请求的
时候，这个标识会作为请求头参数中的User-Agent头域值被发送到服务器。比如使用 Python 中的 Requests
库向服务器发起 HTTP 请求时，服务器读取的User-Agent值为：

```python
python-requests/2.21.0
Java/1.8.0_275
```

​	除了User-Agent之外，常见的用于反爬虫的头域还有 Host 和 Referer。可以有效地屏蔽长期无人维护的爬虫程序，也可以将一些爬虫初学者发起的网络请求拒之门外，但是对于一些经验丰富的爬虫工程师，会被轻松破解

**应对方法**

- 爬虫程序设置合适的请求头来模拟浏览器的访问

##### Cookie反爬

​	Cookie 反爬指的是服务器端通过校验请求头中的 Cookie 值来区分正常用户和爬虫程序的手段.

​	大部分的爬虫程序在默认情况下只请求 HTML 文本资源，这意味着它们并不会主动完成浏览器保存 Cookie 的操作，这次的反爬虫正是利用了这个特点.

![](http://siriushsia.gitee.io/pictures/flow/antiSpyderflow.jpg)



​	浏览器会自动检查响应头中是否存在 Set-Cookie 头域，如果存在，则将值保存在本地，而且往后的每次请求都会自动携带对应的 Cookie 值，这时候只要服务器端对请求头中的 Cookie 值进行校验即可。服务器会校验每个请求头中的 Cookie 值是否符合规则，如果通过校验，则返回正常资源，否则将请求重定向到首页，同时在响应头中添加 Set-Cookie 头域和 Cookie 值。

**和JavaScript结合**

引入一个 JavaScript 文件，并且在这个文件中实现随机字符串生成和 Cookie 设置的功能，那么服务器端只需要校验 Cookie 值的规则即可。

#### 2.IP访问频率限制

​	服务端可以增加对ip访问频率的限制，当超过频率就可以认为是爬虫

**应对方法**

- 构造自己的 IP 代理池，然后每次访问时随机选择代理（一些 IP 地址不是非常稳定，需要经常检查更新）

#### 3.蜜罐技术

​	在网页中设置一些隐藏的链接，这些链接在浏览器上是看不到的，所以用户也是点击不到的。但是爬虫有可能会点击到。如果有请求点击到了，可以认为是爬虫

##### **应对方法**

- 需要手动去研究网页的内容了，找出这些链接的规律然后排除

#### 4.验证码，滑动解锁之类

​	需要用户输入验证码或者滑动解锁以后才能进行登陆

**应对方法**

- 验证码的话，可以用机器学习图像识别来训练验证码识别模型
- 滑动解锁可以考虑使用selenium结合图像识别来自动滑动滑块实现滑动解锁

#### 5.设备指纹

**应用场景**

- 欺诈风向
- 身份伪造
- 定向营销
- 支付交易安全

**原理**

通过收集硬件信息，软件信息，环境信息还有网络信息等特征信息生成设备DNA指纹提供给客户业务系统调用

> 网易网盾:	https://dun.163.com/product/dna
>
> 阿里云:	https://help.aliyun.com/document_detail/101030.html

#### 6.文本混淆反爬虫-iconfont字体反爬

利用前端页面自定义字体的方式来实现反爬的技术手段。具体使用到是 CSS3 中的自定义字体(**@font-face**)模块，自定义字体主要是实现将自定义的 Web 字体嵌入到指定网页中去。这就导致我们去爬取电影评分时，获取到的返回文本中都是乱码符号。

#### 7.代码混淆

常用的混淆方法有正则替换、代码编码和代码复杂化等。为了防止有心人在浏览器中调试，开发者甚至会
在代码中加入一些能够干扰调试的代码。

##### AST

在[计算机科学](https://baike.baidu.com/item/计算机科学)中，抽象语法树（Abstract Syntax Tree，AST），或简称**语法树**（Syntax tree），是[源代码](https://baike.baidu.com/item/源代码)[语法](https://baike.baidu.com/item/语法)结构的一种抽象表示。它以树状的形式表现[编程语言](https://baike.baidu.com/item/编程语言)的语法结构，树上的每个节点都表示源代码中的一种结构。

###### 使用场景

- 代码语法检查，格式化等-JSLint
- 代码混淆压缩-UglifyJS
- 优化变更代码-webpack
- 语法转译-babel

> ast在线解析网站: https://astexplorer.net/
>
> Github开源js代码混淆工具:https://github.com/javascript-obfuscator/javascript-obfuscator

### 反爬虫的案例

#### 豆瓣

1. 登录分配一个cookie

2. 在没有携带 cookie 的情况下，如果某个 IP 短时间高并发请求网站，该 IP 会立马被封。当 IP 被封，登录豆瓣网站会解封。
3. 在携带 cookie 的情况下，某个 IP 请求网站过于频繁。豆瓣的反爬虫机制变为只封 cookie 不封 IP。也就说退出登录或者换个账号还能继续访问网站。

#### 美团猫眼电影&大众点评

​	利用前端页面自定义字体的方式来实现反爬的技术手段。具体使用的是 CSS3 中的自定义字体(**@font-face**)模块，自定义字体主要是实现将自定义的 Web 字体嵌入到指定网页中去。这就导致我们去爬取电影评分时，获取到的返回文本中都是乱码符号。

> 猫眼电影：https://maoyan.com/films/1334342
>
> 大众点评：http://www.dianping.com/shop/l3LjF3FB4zC6CDrZ

#### 抖音

**抖音早期as和cp字段分析**

1. 时间戳转十六进制
2. 将时间戳排序俩次，
   a1 v3 是排序key
   sprintf(byte_102323F30, "%08x", a1);
   sprintf(byte_102323F3A, "%08x", v3);
3. 将url参数用MD5加密一次或俩次根据时间戳&运算
4. 将第一次排序结果写入前16位地址加一写入（从1插入），隔一位插入，前边拼a1
5. 将第二次排序结果写入后16位（从0插入）后边拼e1

**抖音现在加密字段**

抖音每一次 HTTP 请求，都会带上 x-khronos 和 x-gorgon 这两个参数。抖音会在每次请求中校验这两个参数，如果校验不通过，那么请求就无效。 

- x-khronos，是标识请求时间，采用 Unix 时间格式。

- x-gorgon，它是cookie,X-SS-STUB,X-Khronos,Url进行混合加密之后的参数

> 参考分析记录：https://segmentfault.com/a/1190000038326422

## 总结

#### 参数加密问题

- 加了js脚本的网页会对所有请求的参数加密，但并不是所有的接口都有服务端SDK解密逻辑，导致一些不需要加密的接口需要进行修改，代价过高。

#### 效率问题

- RSA加密在服务端进行解密的性能消耗比较大，ROI低。

#### 部署问题

##### 新旧版本升级问题

9台服务器，服务端SDK没有在全部9台服务器上部署完但是前端SDK已经部署了一台服务器，会导致有些服务器收到自己并不能解密的token。好比如我拿着钥匙解密但是你给了我一把数字密码锁

1. 服务端负责解密的逻辑先部署, 一台一台的部署到所有服务器上
2. 负责解密的逻辑部署到所有服务器后再上前端负责加密token的js脚本等
3. 等到一段时间后, 用户大多都是新的解密逻辑后,更改配置文件,只接受最新版本的token

##### token开启版本限制策略产生的问题

- 对接口进行逐步放开的策略，根据kibana日志确认某一接口没有相关的服务方调用就开始限制加密token的版本

#### 信息收集问题

1. 通过tkver字段的算法版本的覆盖率来判断新版加密逻辑有多少更新
2. 通过JS_VER_ERR来认定是否是被token版本拦截下来的

